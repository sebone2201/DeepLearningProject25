{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYQiprE0pcVD"
      },
      "source": [
        "# CLIP zero-shot Evaluation\n",
        "This short notebook implements the dataset split into base and novel categories (see project assignment) and runs the zero-shot evaluation with CLIP.\n",
        "Feel free to copy the code contained in this notebook or to directly use this notebook as starting point for you project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "UzXtFjhh7iOS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6ffc5a6-9d92-4bf2-dc0e-30464a438494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai_clip\n",
            "  Downloading openai-clip-1.0.1.tar.gz (1.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from openai_clip)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from openai_clip) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai_clip) (4.67.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->openai_clip) (0.2.13)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai_clip\n",
            "  Building wheel for openai_clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai_clip: filename=openai_clip-1.0.1-py3-none-any.whl size=1368605 sha256=020c7cc13c2d80c2a18ec64f58fb200d02fee356069c626e3968fa9ac6bb8523\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/17/90/042948fd2e2a87f1dcf6db6d438cad015c49db0c53d1d9c7dc\n",
            "Successfully built openai_clip\n",
            "Installing collected packages: ftfy, openai_clip\n",
            "Successfully installed ftfy-6.3.1 openai_clip-1.0.1\n"
          ]
        }
      ],
      "source": [
        "# we need to install clip as it is not pre-installed\n",
        "# you are also free to use open_clip which provide more models\n",
        "# https://github.com/mlfoundations/open_clip\n",
        "%pip install openai_clip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QtqdSOr8qqOn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import clip\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2353MHw1p24h"
      },
      "source": [
        "## Dataset Loading\n",
        "Let's get the data directly from torchvision as we have seen during labs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M_1CrUhZpVCq"
      },
      "outputs": [],
      "source": [
        "def get_data(data_dir=\"./data\", transform=None):\n",
        "    \"\"\"Load Flowers102 train, validation and test sets.\n",
        "    Args:\n",
        "        data_dir (str): Directory where the dataset will be stored.\n",
        "        transform (torch.Compose)\n",
        "    Returns:\n",
        "        tuple: A tuple containing the train, validation, and test sets.\n",
        "    \"\"\"\n",
        "    train = torchvision.datasets.Flowers102(root=data_dir, split=\"train\", download=True, transform=transform)\n",
        "    val = torchvision.datasets.Flowers102(root=data_dir, split=\"val\", download=True, transform=transform)\n",
        "    test = torchvision.datasets.Flowers102(root=data_dir, split=\"test\", download=True, transform=transform)\n",
        "    return train, val, test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJI_a5EizA5a"
      },
      "source": [
        "## Base and Novel categories\n",
        "To split in base and novel categories we list all dataset classes, and count their number (we already know it's 102 but let's do it properly).\n",
        "Then, we just allocate the first half to base categories and the remaining half to novel ones.\n",
        "We can do this because we are simulating a real world application, but keep in mind this will not happen out there!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nfq51vd8q_5a"
      },
      "outputs": [],
      "source": [
        "def base_novel_categories(dataset):\n",
        "    # set returns the unique set of all dataset classes\n",
        "    all_classes = set(dataset._labels)\n",
        "    # and let's count them\n",
        "    num_classes = len(all_classes)\n",
        "\n",
        "    # here list(range(num_classes)) returns a list from 0 to num_classes - 1\n",
        "    # then we slice the list in half and generate base and novel category lists\n",
        "    base_classes = list(range(num_classes))[:num_classes//2]\n",
        "    novel_classes = list(range(num_classes))[num_classes//2:]\n",
        "    return base_classes, novel_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvDdoYQr2fIu"
      },
      "source": [
        "## Inspect Classes\n",
        "Let's now visualize which are the base and novel classes.\n",
        "To do so, we first get a dummy test set (without augmentations) as we are just interested in the dataset labels. Then, we split it useing `base_novel_categories`.\n",
        "Finally, we use the hard-coded CLASS_NAMES to print the class in natural language.\n",
        "\n",
        "> Note: the list of class names was only recently added to `torchvision.datasets.Flowers102`. To avoid useless errors that can occour to you, we decided to also provide such a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "veGGpNDctCgR",
        "outputId": "bf03bf0f-4ef2-470f-8fd6-9ef0e0cdd4cd",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base Class Names: [(0, 'pink primrose'), (1, 'hard-leaved pocket orchid'), (2, 'canterbury bells'), (3, 'sweet pea'), (4, 'english marigold'), (5, 'tiger lily'), (6, 'moon orchid'), (7, 'bird of paradise'), (8, 'monkshood'), (9, 'globe thistle'), (10, 'snapdragon'), (11, \"colt's foot\"), (12, 'king protea'), (13, 'spear thistle'), (14, 'yellow iris'), (15, 'globe-flower'), (16, 'purple coneflower'), (17, 'peruvian lily'), (18, 'balloon flower'), (19, 'giant white arum lily'), (20, 'fire lily'), (21, 'pincushion flower'), (22, 'fritillary'), (23, 'red ginger'), (24, 'grape hyacinth'), (25, 'corn poppy'), (26, 'prince of wales feathers'), (27, 'stemless gentian'), (28, 'artichoke'), (29, 'sweet william'), (30, 'carnation'), (31, 'garden phlox'), (32, 'love in the mist'), (33, 'mexican aster'), (34, 'alpine sea holly'), (35, 'ruby-lipped cattleya'), (36, 'cape flower'), (37, 'great masterwort'), (38, 'siam tulip'), (39, 'lenten rose'), (40, 'barbeton daisy'), (41, 'daffodil'), (42, 'sword lily'), (43, 'poinsettia'), (44, 'bolero deep blue'), (45, 'wallflower'), (46, 'marigold'), (47, 'buttercup'), (48, 'oxeye daisy'), (49, 'common dandelion'), (50, 'petunia')]\n",
            "Novel Class Names: [(51, 'wild pansy'), (52, 'primula'), (53, 'sunflower'), (54, 'pelargonium'), (55, 'bishop of llandaff'), (56, 'gaura'), (57, 'geranium'), (58, 'orange dahlia'), (59, 'pink-yellow dahlia?'), (60, 'cautleya spicata'), (61, 'japanese anemone'), (62, 'black-eyed susan'), (63, 'silverbush'), (64, 'californian poppy'), (65, 'osteospermum'), (66, 'spring crocus'), (67, 'bearded iris'), (68, 'windflower'), (69, 'tree poppy'), (70, 'gazania'), (71, 'azalea'), (72, 'water lily'), (73, 'rose'), (74, 'thorn apple'), (75, 'morning glory'), (76, 'passion flower'), (77, 'lotus'), (78, 'toad lily'), (79, 'anthurium'), (80, 'frangipani'), (81, 'clematis'), (82, 'hibiscus'), (83, 'columbine'), (84, 'desert-rose'), (85, 'tree mallow'), (86, 'magnolia'), (87, 'cyclamen'), (88, 'watercress'), (89, 'canna lily'), (90, 'hippeastrum'), (91, 'bee balm'), (92, 'ball moss'), (93, 'foxglove'), (94, 'bougainvillea'), (95, 'camellia'), (96, 'mallow'), (97, 'mexican petunia'), (98, 'bromelia'), (99, 'blanket flower'), (100, 'trumpet creeper'), (101, 'blackberry lily')]\n"
          ]
        }
      ],
      "source": [
        "from os import name\n",
        "_, _, tmp_test = get_data()\n",
        "base_classes, novel_classes = base_novel_categories(tmp_test)\n",
        "CLASS_NAMES = [\"pink primrose\", \"hard-leaved pocket orchid\", \"canterbury bells\", \"sweet pea\", \"english marigold\", \"tiger lily\", \"moon orchid\", \"bird of paradise\", \"monkshood\", \"globe thistle\", \"snapdragon\", \"colt's foot\", \"king protea\", \"spear thistle\", \"yellow iris\", \"globe-flower\", \"purple coneflower\", \"peruvian lily\", \"balloon flower\", \"giant white arum lily\", \"fire lily\", \"pincushion flower\", \"fritillary\", \"red ginger\", \"grape hyacinth\", \"corn poppy\", \"prince of wales feathers\", \"stemless gentian\", \"artichoke\", \"sweet william\", \"carnation\", \"garden phlox\", \"love in the mist\", \"mexican aster\", \"alpine sea holly\", \"ruby-lipped cattleya\", \"cape flower\", \"great masterwort\", \"siam tulip\", \"lenten rose\", \"barbeton daisy\", \"daffodil\", \"sword lily\", \"poinsettia\", \"bolero deep blue\", \"wallflower\", \"marigold\", \"buttercup\", \"oxeye daisy\", \"common dandelion\", \"petunia\", \"wild pansy\", \"primula\", \"sunflower\", \"pelargonium\", \"bishop of llandaff\", \"gaura\", \"geranium\", \"orange dahlia\", \"pink-yellow dahlia?\", \"cautleya spicata\", \"japanese anemone\", \"black-eyed susan\", \"silverbush\", \"californian poppy\", \"osteospermum\", \"spring crocus\", \"bearded iris\", \"windflower\", \"tree poppy\", \"gazania\", \"azalea\", \"water lily\", \"rose\", \"thorn apple\", \"morning glory\", \"passion flower\", \"lotus\", \"toad lily\", \"anthurium\", \"frangipani\", \"clematis\", \"hibiscus\", \"columbine\", \"desert-rose\", \"tree mallow\", \"magnolia\", \"cyclamen\", \"watercress\", \"canna lily\", \"hippeastrum\", \"bee balm\", \"ball moss\", \"foxglove\", \"bougainvillea\", \"camellia\", \"mallow\", \"mexican petunia\", \"bromelia\", \"blanket flower\", \"trumpet creeper\", \"blackberry lily\"]\n",
        "print(\"Base Class Names:\", [(i, CLASS_NAMES[i]) for i in base_classes])\n",
        "name_base = [CLASS_NAMES[i] for i in base_classes]\n",
        "name_novel = [CLASS_NAMES[i] for i in novel_classes]\n",
        "print(\"Novel Class Names:\", [(i, CLASS_NAMES[i]) for i in novel_classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8puO1VNpzwvi"
      },
      "source": [
        "## Split Dataset\n",
        "The next step is to actually split the dataset into the base and novel categories we extract from `base_novel_categories`.\n",
        "To split the data we need the dataset (obviously) and the list of base classes. If the sample label is not part of the base categories, then it must be part of the novel ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "msOszMs2zRRu"
      },
      "outputs": [],
      "source": [
        "def split_data(dataset, base_classes):\n",
        "    # these two lists will store the sample indexes\n",
        "    base_categories_samples = []\n",
        "    novel_categories_samples = []\n",
        "\n",
        "    # we create a set of base classes to compute the test below in O(1)\n",
        "    # this is optional and can be removed\n",
        "    base_set = set(base_classes)\n",
        "\n",
        "    # here we iterate over sample labels and also get the correspondent sample index\n",
        "    for sample_id, label in enumerate(dataset._labels):\n",
        "        if label in base_set:\n",
        "            base_categories_samples.append(sample_id)\n",
        "        else:\n",
        "            novel_categories_samples.append(sample_id)\n",
        "\n",
        "    # here we create the dataset subsets\n",
        "    # the torch Subset is just a wrapper around the dataset\n",
        "    # it simply stores the subset indexes and the original dataset (your_subset.dataset)\n",
        "    # when asking for sample i in the subset, torch will look for its original position in the dataset and retrieve it\n",
        "    # https://pytorch.org/docs/stable/data.html#torch.utils.data.Subset\n",
        "    base_dataset = torch.utils.data.Subset(dataset, base_categories_samples)\n",
        "    novel_dataset = torch.utils.data.Subset(dataset, novel_categories_samples)\n",
        "    return base_dataset, novel_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQZT22rE8hBw"
      },
      "source": [
        "## Extract k shots\n",
        "As the dataset already provides 10 train and validation shots, we do not need to extract them.\n",
        "Beaware that Few-Shot Adaptation papers must do this operation as most datasets count significantly more samples in both the training and validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KpbPRLr7WL_"
      },
      "source": [
        "## Load CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6uLZRT7YJx",
        "outputId": "75a6c398-50a3-4f10-e8e1-d9b3f5e2b5b7",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:04<00:00, 76.4MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Compose(\n",
              "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=True)\n",
              "    CenterCrop(size=(224, 224))\n",
              "    <function _convert_image_to_rgb at 0x7ba18649fb00>\n",
              "    ToTensor()\n",
              "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
        "model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
        "\n",
        "# preprocess contains CLIP's pre-defined augmentations, let's inspect them!\n",
        "preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9H14899ses"
      },
      "source": [
        "## Load and Prepare Data\n",
        "Here we get the three dataset split and pass clip pre-defined augmentations.\n",
        "Then, we compute base and novel categories (in this case is redundand as we already did it before).\n",
        "Finally, se split the three datasets into base and novel categories.\n",
        "As we want to use the novel categories only for the test set, we drop `train_novel` and `val_novel`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "TVrYUYTv9ttM",
        "outputId": "ea45fde2-c4a4-44fe-8c98-e62c688081ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 345M/345M [00:14<00:00, 24.2MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 502/502 [00:00<00:00, 1.70MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15.0k/15.0k [00:00<00:00, 47.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# get the three datasets\n",
        "train_set, val_set, test_set = get_data(transform=preprocess)\n",
        "\n",
        "# split classes into base and novel\n",
        "base_classes, novel_classes = base_novel_categories(train_set)\n",
        "\n",
        "# split the three datasets\n",
        "train_base, _ = split_data(train_set, base_classes)\n",
        "val_base, _ = split_data(val_set, base_classes)\n",
        "test_base, test_novel = split_data(test_set, base_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcgMwr3J9VIg"
      },
      "source": [
        "## Compute Zero-Shot Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uhblkvm9US4",
        "outputId": "bda519d1-930c-44e8-ace8-c55868ac8998"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:19<00:00,  1.04it/s]\n",
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:28<00:00,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Base classes accuracy: 71.33%\n",
            "üîç Novel classes accuracy: 78.24%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad() # we don't want gradients\n",
        "def eval(model, dataset, categories, batch_size, device, label=\"\"):\n",
        "    # let's set the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Remap labels into a contiguous set starting from zero\n",
        "    contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "    # here we apply the standard CLIP template used for oxford flowers to all categories\n",
        "    # and immediately tokenize each sentence (convert natural language into numbers - feel free to print the text input to inspect them)\n",
        "    text_inputs = clip.tokenize(\n",
        "        [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]\n",
        "    ).to(device)\n",
        "\n",
        "    # we can encode the text features once as they are shared for all images\n",
        "    # therefore we do it outside the evaluation loop\n",
        "    text_features = model.encode_text(text_inputs)\n",
        "    # and here we normalize them (standard pratice with CLIP)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True) # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "    # simple dataloader creation\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "    for image, target in tqdm(dataloader, desc=label):\n",
        "        # base categories range from 0 to 50, whil novel ones from 51 to 101\n",
        "        # therefore we must map categories to the [0, 50], otherwise we will have wrong predictions\n",
        "        # Map targets in contiguous set starting from zero\n",
        "        # Labels needs to be .long() in pytorch\n",
        "        target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "        image = image.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # forward image through CLIP image encoder\n",
        "        image_features = model.encode_image(image)\n",
        "        # and normalize\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "        predicted_class = (image_features @ text_features.T).argmax(dim=-1)\n",
        "        # now we check which are correct, and sum them (False == 0, True == 1)\n",
        "        correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    # and now we compute the accuracy\n",
        "    accuracy = correct_predictions / len(dataset)\n",
        "    return accuracy\n",
        "\n",
        "base_accuracy = eval(model=model, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "\n",
        "print()\n",
        "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baYfLKNdfbUR"
      },
      "source": [
        "## Harmonic Mean\n",
        "Few-Shot Adaptations papers usually report the Harmonic Mean.\n",
        "The harmonic mean tends to mitigate the impact of large outliers (base accuracy) and aggravate the impact of small ones (novel accuracy).\n",
        "Thus, achieving very high base accuracies at the expense of the novel accuracy will be penalized by the HM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKAXR7hlfbUR",
        "outputId": "e00e50f4-3b0f-4e79-ed09-e8e82cc53668"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Harmonic Mean: 74.62%\n"
          ]
        }
      ],
      "source": [
        "def harmonic_mean(base_accuracy, novel_accuracy):\n",
        "    numerator = 2\n",
        "    denominator = 1 / base_accuracy + 1 / novel_accuracy\n",
        "    hm = numerator / denominator\n",
        "    return hm\n",
        "\n",
        "print(f\"üîç Harmonic Mean: {harmonic_mean(base_accuracy, novel_accuracy)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Augmentation"
      ],
      "metadata": {
        "id": "R08jwnaQ8R2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Define data augmentation transformations\n",
        "augmentation_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)),  # Random crop and resize\n",
        "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance to flip horizontally\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Random color adjustments\n",
        "    transforms.RandomSolarize(0.5, p=1),\n",
        "    transforms.RandomRotation(degrees=15),  # Rotate image within ¬±15 degrees\n",
        "    #transforms.ToTensor(),  # Convert PIL image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for CLIP\n",
        "])\n",
        "\n",
        "# Validation transformations (no augmentation, just normalization)\n",
        "validation_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to a fixed size\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "# Example dataset class\n",
        "class AugmentedImageTextDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data  # Assume data is a list of (image_path, label) tuples\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, label = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "          image = self.transform(image)\n",
        "        return image, label\n",
        "augmented_train_base = AugmentedImageTextDataset(data = train_base, transform = augmentation_transforms)"
      ],
      "metadata": {
        "id": "6OA3pSfm8QFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning of the textual linear layer\n",
        "\n",
        "We fine-tune the last linear layer of the textual encoder for the classification of the base train data."
      ],
      "metadata": {
        "id": "eQPAGfZ9y-8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def fine_tuning_linear_text(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([text_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "  print(\"üß† Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_text_layer = fine_tuning_linear_text(model=model, train_dataset=train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=16, num_epochs=10, device=device)"
      ],
      "metadata": {
        "id": "vtwN7HP-w8of",
        "outputId": "cd49974d-f14d-4981-ffc7-d3c9ee00b0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Fine-tuning training+validation on Base Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/32 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CLASS_NAMES' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-00cdcc871c9d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m \u001b[0mmodel_ft_text_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuning_linear_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-00cdcc871c9d>\u001b[0m in \u001b[0;36mfine_tuning_linear_text\u001b[0;34m(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           text_inputs = clip.tokenize(\n\u001b[0;32m---> 46\u001b[0;31m           [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-00cdcc871c9d>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m           text_inputs = clip.tokenize(\n\u001b[0;32m---> 46\u001b[0;31m           [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CLASS_NAMES' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we evaluate the fine-tuned CLIP in both base test set (few-shot evaluation) and novel test set (zero-shot evaluation)."
      ],
      "metadata": {
        "id": "jnmc4AyAzMVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = eval(model=model_ft_text_layer, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"üß† Few-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model_ft_text_layer, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "print()\n",
        "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "h5podH53zHc7",
        "outputId": "d64f8cef-6aff-4581-c548-70a1eef37494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Few-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:31<00:00,  1.58s/it]\n",
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:45<00:00,  1.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Base classes accuracy: 92.72%\n",
            "üîç Novel classes accuracy: 63.85%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuning of the visual linear layer\n",
        "\n",
        "We fine-tune the last linear layer of the visual encoder for the classification of the base train data."
      ],
      "metadata": {
        "id": "RDz9XS0DhxnL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfzZ1Es-MwF5",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "9da4df7b-ed9c-448a-816f-52b9d087756e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "fine_tuning_linear_visual() got an unexpected keyword argument 'augmented_train_dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0e7c28c39bca>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m \u001b[0mmodel_ft_visual_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuning_linear_visual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maugmented_train_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: fine_tuning_linear_visual() got an unexpected keyword argument 'augmented_train_dataset'"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def fine_tuning_linear_visual(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  visual_projection = model.visual.proj\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  visual_projection.requires_grad = True\n",
        "  #text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([visual_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"üß† Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_visual_layer = fine_tuning_linear_visual(model=model, train_dataset=augmented_train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=16, num_epochs=30, device=device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we evaluate the fine-tuned CLIP in both base test set (few-shot evaluation) and novel test set (zero-shot evaluation)."
      ],
      "metadata": {
        "id": "xzrUWPcwu00p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = eval(model=model_ft_visual_layer, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"üß† Few-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model_ft_visual_layer, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "print()\n",
        "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "h1Xyn8_hldt5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04b32264-8156-43c1-cb11-eece01111142"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Few-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:31<00:00,  1.59s/it]\n",
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:46<00:00,  1.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Base classes accuracy: 82.81%\n",
            "üîç Novel classes accuracy: 52.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using CLIP original loss"
      ],
      "metadata": {
        "id": "mY4XTSFHxDyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "temperature = 0.07\n",
        "\n",
        "def fine_tuning_linear_visual(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  visual_projection = model.visual.proj\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for param in model.parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  # Unfreeze the projection layer\n",
        "  visual_projection.requires_grad = True\n",
        "  #text_projection.requires_grad = True\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam([visual_projection], lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"üß† Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in target]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "\n",
        "          target = torch.arange(len(target))\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = (image_features @ text_features.T)* torch.exp(torch.tensor([temperature]).to(device))\n",
        "\n",
        "          loss = (criterion(logits, target) +criterion(logits.T, target))/2\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_visual_layer = fine_tuning_linear_visual(model=model, train_dataset=train_base, val_dataset=val_base, categories=base_classes, lr = 0.00001, batch_size=16, num_epochs=20, device=device)\n"
      ],
      "metadata": {
        "id": "ZKAQ6HtQxBkn",
        "outputId": "23c54246-a6a6-4fbd-f3da-8ee0038b2995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Fine-tuning training+validation on Base Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 2.669394940137863; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  2.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation loss: 3.8241214007139206; Validation accuracy: 73.14%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training loss: 2.6547181755304337; Training accuracy: 70.20%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Validation loss: 3.809515818953514; Validation accuracy: 73.33%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training loss: 2.6390961185097694; Training accuracy: 70.39%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Validation loss: 3.7945312410593033; Validation accuracy: 71.96%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training loss: 2.6234960108995438; Training accuracy: 70.59%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Validation loss: 3.780126266181469; Validation accuracy: 70.20%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training loss: 2.6073362827301025; Training accuracy: 70.59%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Validation loss: 3.766261711716652; Validation accuracy: 68.63%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training loss: 2.595405198633671; Training accuracy: 69.41%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Validation loss: 3.7536500096321106; Validation accuracy: 68.24%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training loss: 2.5816031396389008; Training accuracy: 69.80%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Validation loss: 3.741886779665947; Validation accuracy: 67.25%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training loss: 2.5735903084278107; Training accuracy: 70.20%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Validation loss: 3.731088675558567; Validation accuracy: 67.45%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training loss: 2.5596833154559135; Training accuracy: 72.35%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Validation loss: 3.721024088561535; Validation accuracy: 67.06%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training loss: 2.5493284538388252; Training accuracy: 70.78%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Validation loss: 3.711524799466133; Validation accuracy: 66.27%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training loss: 2.5419431999325752; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Validation loss: 3.7026090547442436; Validation accuracy: 67.06%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training loss: 2.5296263098716736; Training accuracy: 71.96%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Validation loss: 3.6941885501146317; Validation accuracy: 67.06%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training loss: 2.522714301943779; Training accuracy: 72.16%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Validation loss: 3.686231814324856; Validation accuracy: 67.06%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training loss: 2.5152296647429466; Training accuracy: 73.33%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Validation loss: 3.6786944568157196; Validation accuracy: 66.67%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:08<00:00,  3.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training loss: 2.5057100653648376; Training accuracy: 76.27%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Validation loss: 3.671561114490032; Validation accuracy: 67.06%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training loss: 2.4966844469308853; Training accuracy: 73.73%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Validation loss: 3.6647297367453575; Validation accuracy: 67.65%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training loss: 2.4932354390621185; Training accuracy: 71.76%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Validation loss: 3.658332198858261; Validation accuracy: 67.65%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Training loss: 2.483542248606682; Training accuracy: 75.49%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18, Validation loss: 3.652178570628166; Validation accuracy: 68.04%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Training loss: 2.4799063205718994; Training accuracy: 73.92%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19, Validation loss: 3.6463252305984497; Validation accuracy: 68.63%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:07<00:00,  4.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Training loss: 2.4733694940805435; Training accuracy: 75.69%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:10<00:00,  3.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20, Validation loss: 3.640775181353092; Validation accuracy: 69.41%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_accuracy = eval(model=model_ft_visual_layer, dataset=test_base, categories=base_classes, batch_size=128, device=device, label=\"üß† Few-shot evaluation on Base Classes\")\n",
        "novel_accuracy = eval(model=model_ft_visual_layer, dataset=test_novel, categories=novel_classes, batch_size=128, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "print()\n",
        "print(f\"üîç Base classes accuracy: {base_accuracy*100:.2f}%\")\n",
        "print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "PKYSPNnjzRIc",
        "outputId": "ae7f8687-7362-42a8-c07b-c8ba8044c4ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Few-shot evaluation on Base Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:29<00:00,  1.47s/it]\n",
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29/29 [00:43<00:00,  1.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Base classes accuracy: 69.07%\n",
            "üîç Novel classes accuracy: 63.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Layer Norm"
      ],
      "metadata": {
        "id": "UUi_DPPm8Iw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "def fine_tuning_linear_visual(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "  model = model.float()\n",
        "\n",
        "  visual_projection = model.visual.proj\n",
        "  text_projection = model.text_projection\n",
        "\n",
        "  # Freeze all parameters in the model\n",
        "  for name, param in model.named_parameters():\n",
        "    if \"ln\" not in name:  # Check for LayerNorm parameters\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "  trainable_params = [param for name, param in model.named_parameters() if param.requires_grad]\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(trainable_params, lr=lr)\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"üß† Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "          text_inputs = clip.tokenize(\n",
        "          [f\"a photo of a {CLASS_NAMES[c]}, a type of flower.\" for c in categories]).to(device)\n",
        "\n",
        "          text_features = model.encode_text(text_inputs).float()\n",
        "          # and here we normalize them (standard pratice with CLIP)\n",
        "          text_feature_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "          text_features = text_features/text_feature_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          image_features = model.encode_image(image).float()\n",
        "          # and normalize\n",
        "          image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "          image_features = image_features / image_features_norm # per avere norma 1 per calcolare cosine similarity\n",
        "\n",
        "          # here cosine similarity between image and text features and keep the argmax for every row (every image)\n",
        "          logits = image_features @ text_features.T\n",
        "\n",
        "          loss = criterion(logits, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = logits.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    novel_accuracy = eval(model=model, dataset=test_novel, categories=novel_classes, batch_size=32, device=device, label=\"üß† Zero-shot evaluation on Novel Classes\")\n",
        "    print(f\"üîç Novel classes accuracy: {novel_accuracy*100:.2f}%\")\n",
        "    print()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "model_ft_visual_layer = fine_tuning_linear_visual(model=model, train_dataset=train_base, val_dataset=val_base, categories=base_classes, lr = 0.0001, batch_size=32, num_epochs=30, device=device)"
      ],
      "metadata": {
        "id": "4UDK7UO_8Gh0",
        "outputId": "41181a30-e833-485a-b531-6e9afc344801",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Fine-tuning training+validation on Base Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:18<00:00,  1.17s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 3.836524084210396; Training accuracy: 69.02%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation loss: 3.8312969505786896; Validation accuracy: 71.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 78.35%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:18<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Training loss: 3.8295159488916397; Training accuracy: 69.41%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2, Validation loss: 3.8246214538812637; Validation accuracy: 72.16%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 78.37%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Training loss: 3.82290743291378; Training accuracy: 69.22%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3, Validation loss: 3.818163588643074; Validation accuracy: 73.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 78.16%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Training loss: 3.816444754600525; Training accuracy: 70.20%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4, Validation loss: 3.811863273382187; Validation accuracy: 72.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.97%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Training loss: 3.810079589486122; Training accuracy: 70.59%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5, Validation loss: 3.8056256771087646; Validation accuracy: 72.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.88%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Training loss: 3.8037253618240356; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6, Validation loss: 3.7994285374879837; Validation accuracy: 72.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.88%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.10s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Training loss: 3.797354355454445; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7, Validation loss: 3.793176233768463; Validation accuracy: 72.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.69%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Training loss: 3.7909667789936066; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8, Validation loss: 3.7869236767292023; Validation accuracy: 72.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.58%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:18<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Training loss: 3.7844999879598618; Training accuracy: 70.98%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9, Validation loss: 3.78067210316658; Validation accuracy: 73.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.58%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Training loss: 3.778153195977211; Training accuracy: 71.18%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Validation loss: 3.7743978649377823; Validation accuracy: 73.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.45%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Training loss: 3.7716043144464493; Training accuracy: 72.55%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11, Validation loss: 3.7680599689483643; Validation accuracy: 73.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 77.23%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Training loss: 3.7650596499443054; Training accuracy: 72.94%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12, Validation loss: 3.7617155462503433; Validation accuracy: 73.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.93%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Training loss: 3.758386805653572; Training accuracy: 73.73%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13, Validation loss: 3.75533726811409; Validation accuracy: 73.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.96%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Training loss: 3.751645013689995; Training accuracy: 73.73%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14, Validation loss: 3.7488829493522644; Validation accuracy: 73.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.99%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Training loss: 3.7448973804712296; Training accuracy: 74.31%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15, Validation loss: 3.7423578649759293; Validation accuracy: 73.92%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.96%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Training loss: 3.7381471693515778; Training accuracy: 75.29%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16, Validation loss: 3.735743820667267; Validation accuracy: 74.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.74%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:17<00:00,  1.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Training loss: 3.7310997396707535; Training accuracy: 77.25%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:06<00:00,  2.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17, Validation loss: 3.729115381836891; Validation accuracy: 75.69%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "üß† Zero-shot evaluation on Novel Classes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 115/115 [00:42<00:00,  2.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Novel classes accuracy: 76.47%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 9/16 [00:10<00:08,  1.18s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-33ab26f1b2fd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ViT-B/16\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m \u001b[0mmodel_ft_visual_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuning_linear_visual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-33ab26f1b2fd>\u001b[0m in \u001b[0;36mfine_tuning_linear_visual\u001b[0;34m(model, train_dataset, val_dataset, categories, lr, batch_size, num_epochs, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m           \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m           \u001b[0;31m# Backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple prompt tuning"
      ],
      "metadata": {
        "id": "PFABN5Y8l9Ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "from collections import OrderedDict\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "\n",
        "\n",
        "from clip import clip\n",
        "from clip.simple_tokenizer import SimpleTokenizer as _Tokenizer\n",
        "\n",
        "_tokenizer = _Tokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, clip_model):\n",
        "        super().__init__()\n",
        "        self.transformer = clip_model.transformer\n",
        "        self.positional_embedding = clip_model.positional_embedding\n",
        "        self.ln_final = clip_model.ln_final\n",
        "        self.text_projection = clip_model.text_projection\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "\n",
        "    def forward(self, prompts, tokenized_prompts):\n",
        "        x = prompts + self.positional_embedding.type(self.dtype)\n",
        "        x = x.permute(1, 0, 2)  # NLD -> LND\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # LND -> NLD\n",
        "        x = self.ln_final(x).type(self.dtype)\n",
        "\n",
        "        # x.shape = [batch_size, n_ctx, transformer.width]\n",
        "        # take features from the eot embedding (eot_token is the highest number in each sequence)\n",
        "        x = x[torch.arange(x.shape[0]), tokenized_prompts.argmax(dim=-1)] @ self.text_projection\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class PromptLearner(nn.Module):\n",
        "    def __init__(self, clip_model, classnames, new_classnames, n_ctx, ctx_init, class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "        n_cls = len(classnames)\n",
        "        dtype = clip_model.dtype\n",
        "        ctx_dim = clip_model.ln_final.weight.shape[0]\n",
        "        clip_imsize = clip_model.visual.input_resolution\n",
        "        vis_dim = clip_model.visual.output_dim\n",
        "        #self.meta_net = ModulationMLP(input_dim=vis_dim, hidden_dim=vis_dim//2, output_dim=ctx_dim)\n",
        "        self.meta_net = nn.Sequential(OrderedDict([\n",
        "            (\"linear1\", nn.Linear(vis_dim, vis_dim // 16)),\n",
        "            (\"relu\", nn.ReLU(inplace=True)),\n",
        "            (\"linear2\", nn.Linear(vis_dim // 16, ctx_dim))\n",
        "        ]))\n",
        "\n",
        "        # Use given words to initialize context vectors\n",
        "        if ctx_init:\n",
        "            ctx_init = ctx_init.replace(\"_\", \" \")\n",
        "            self.n_ctx = len(ctx_init.split(\" \"))\n",
        "            prompt = clip.tokenize(ctx_init).to(clip_model.token_embedding.weight.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = clip_model.token_embedding(prompt)\n",
        "            ctx_vectors = embedding[0, 1 : 1 + n_ctx, :]\n",
        "            self.prompt_prefix = ctx_init\n",
        "            #we take prompt_prefix as the context that we give\n",
        "        else:\n",
        "            if csc:\n",
        "                print(\"Initializing class-specific contexts\")\n",
        "                self.ctx_vectors = torch.empty(n_cls, n_ctx, ctx_dim)\n",
        "            else:\n",
        "                print(\"Initializing a generic context\")\n",
        "                self.ctx_vectors = torch.empty(n_ctx, ctx_dim)\n",
        "\n",
        "            torch.nn.init.normal_(ctx_vectors, std=0.02)\n",
        "            self.prompt_prefix = \" \".join([\"X\"] * n_ctx)\n",
        "\n",
        "        print(f\"Initial context: '{self.prompt_prefix}'\")\n",
        "        print(f\"Number of context words (tokens): {n_ctx}\")\n",
        "\n",
        "        # These are the `prompts` we want to optimize\n",
        "        self.ctx = nn.Parameter(ctx_vectors) #we don't want this since we want to have just the parameters of the meta net\n",
        "\n",
        "        classnames = [name.replace(\"_\", \" \") for name in classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in classnames]\n",
        "        prompts = [self.prompt_prefix + \" \" + name + \".\" for name in classnames]\n",
        "\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(clip_model.token_embedding.weight.device)\n",
        "        with torch.no_grad():\n",
        "            embedding = clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "\n",
        "        # These token vectors will be saved when in save_model(),\n",
        "        # but they should be ignored in load_model() as we want to use\n",
        "        # those computed using the current class names\n",
        "        self.register_buffer(\"token_prefix\", embedding[:, :1, :])  # SOS\n",
        "        self.register_buffer(\"token_suffix\", embedding[:, 1 + n_ctx :, :])  # CLS, EOS\n",
        "        self.new_classnames = new_classnames\n",
        "        self.clip_model = clip_model\n",
        "        #self.tokenized_prompts = tokenized_prompts\n",
        "        self.n_cls = n_cls\n",
        "        self.n_ctx = n_ctx\n",
        "        self.tokenized_prompts = tokenized_prompts\n",
        "        self.name_lens = name_lens\n",
        "        self.class_token_position = class_token_position\n",
        "\n",
        "    def construct_prompts(self, ctx, prefix, suffix, label=None):\n",
        "        # dim0 is either batch_size (during training) or n_cls (during testing)\n",
        "        # ctx: context tokens, with shape of (dim0, n_ctx, ctx_dim)\n",
        "        # prefix: the sos token, with shape of (n_cls, 1, ctx_dim)\n",
        "        # suffix: remaining tokens, with shape of (n_cls, *, ctx_dim)\n",
        "\n",
        "        if label is not None:\n",
        "            prefix = prefix[label]\n",
        "            suffix = suffix[label]\n",
        "\n",
        "        prompts = torch.cat(\n",
        "            [\n",
        "                prefix,  # (dim0, 1, dim)\n",
        "                ctx,     # (dim0, n_ctx, dim)\n",
        "                suffix,  # (dim0, *, dim)\n",
        "            ],\n",
        "            dim=1,\n",
        "        )\n",
        "        return prompts\n",
        "\n",
        "    def forward(self, im_features, base = True):\n",
        "     if base:\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "        bias = self.meta_net(im_features)\n",
        "        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)\n",
        "        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)\n",
        "        ctx_shifted = ctx + bias            # (batch, n_ctx, ctx_dim)\n",
        "\n",
        "        prompts = []\n",
        "        for ctx_shifted_i in ctx_shifted:\n",
        "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
        "            prompts.append(pts_i)\n",
        "        prompts = torch.stack(prompts)\n",
        "        #print(prompts.shape)\n",
        "\n",
        "\n",
        "        return prompts\n",
        "     else:\n",
        "        classnames = [name.replace(\"_\", \" \") for name in self.new_classnames]\n",
        "        name_lens = [len(_tokenizer.encode(name)) for name in self.new_classnames]\n",
        "        prompts = [self.prompt_prefix + \" \" + name + \".\" for name in self.new_classnames]\n",
        "\n",
        "\n",
        "        tokenized_prompts = torch.cat([clip.tokenize(p) for p in prompts]).to(self.clip_model.token_embedding.weight.device)\n",
        "\n",
        "        embedding = self.clip_model.token_embedding(tokenized_prompts).type(dtype)\n",
        "        prefix = self.token_prefix\n",
        "        suffix = self.token_suffix\n",
        "        ctx = self.ctx\n",
        "        bias = self.meta_net(im_features)\n",
        "        bias = bias.unsqueeze(1)           # (batch, 1, ctx_dim)\n",
        "        ctx = ctx.unsqueeze(0)             # (1, n_ctx, ctx_dim)\n",
        "        ctx_shifted = ctx + bias            # (batch, n_ctx, ctx_dim)\n",
        "\n",
        "        prompts = []\n",
        "        for ctx_shifted_i in ctx_shifted:\n",
        "            ctx_i = ctx_shifted_i.unsqueeze(0).expand(self.n_cls, -1, -1)\n",
        "            pts_i = self.construct_prompts(ctx_i, prefix, suffix)  # (n_cls, n_tkn, ctx_dim)\n",
        "            prompts.append(pts_i)\n",
        "        prompts = torch.stack(prompts)\n",
        "        #print(prompts.shape)\n",
        "\n",
        "\n",
        "        return prompts\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xH78D_nX6wG9"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "class OurCLIP(nn.Module):\n",
        "    def __init__(self, classnames,new_classnames, n_ctx, ctx_init, clip_model,  class_token_position, csc=False):\n",
        "        super().__init__()\n",
        "\n",
        "        # clip_model = clip_model.cpu()\n",
        "        clip_model = clip_model.float()\n",
        "\n",
        "        self.prompt_learner = PromptLearner(clip_model, classnames, new_classnames, n_ctx, ctx_init, class_token_position, csc=csc)\n",
        "        self.tokenized_prompts = self.prompt_learner.tokenized_prompts\n",
        "        self.image_encoder = clip_model.visual\n",
        "        self.text_encoder = TextEncoder(clip_model)\n",
        "        self.logit_scale = clip_model.logit_scale\n",
        "        self.dtype = clip_model.dtype\n",
        "\n",
        "    def forward(self, image, label = None):\n",
        "        tokenized_prompts = self.tokenized_prompts\n",
        "        logit_scale = self.logit_scale.exp()\n",
        "\n",
        "        image_features = self.image_encoder(image.type(self.dtype))\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        prompts = self.prompt_learner(image_features)\n",
        "\n",
        "        logits = []\n",
        "        for pts_i, imf_i in zip(prompts, image_features):\n",
        "            text_features = self.text_encoder(pts_i, tokenized_prompts)\n",
        "            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "            l_i = logit_scale * imf_i @ text_features.t()\n",
        "            logits.append(l_i)\n",
        "        logits = torch.stack(logits)\n",
        "\n",
        "        #if self.prompt_learner.training:\n",
        "        #    return criterion(logits, label)\n",
        "\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "uFAd8yhXyLyM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import Adam\n",
        "import torchvision.transforms as transforms\n",
        "def cocoop( train_dataset, val_dataset, categories,  batch_size, num_epochs, device, n_ctx=4,\n",
        "    ctx_init=\"a photo of a type of flower, the \",\n",
        "    class_token_position=\"end\",\n",
        "    csc=False):\n",
        "\n",
        "  train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "  clip_model, _ = clip.load(\"ViT-B/16\", device=device)\n",
        "  model = OurCLIP(\n",
        "        classnames=name_base, new_classnames= name_novel, n_ctx = n_ctx, ctx_init=ctx_init, clip_model=clip_model, class_token_position=class_token_position, csc=csc\n",
        "    ).to(device)\n",
        "\n",
        "  print(\"Turning off gradients in both the image and the text encoder\")\n",
        "  for name, param in model.named_parameters():\n",
        "        if \"prompt_learner\" not in name:\n",
        "            param.requires_grad_(False)\n",
        "\n",
        "  print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "  print(f\"Total trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  trainable_params = [param for name, param in model.named_parameters() if param.requires_grad]\n",
        "\n",
        "  criterion = torch.nn.CrossEntropyLoss()\n",
        "  optimizer = Adam(trainable_params, lr=0.002,  weight_decay=0.0005)\n",
        "\n",
        "# Verify which parameters are trainable\n",
        "\n",
        "  contig_cat2idx = {cat: idx for idx, cat in enumerate(categories)}\n",
        "\n",
        "\n",
        "\n",
        "  print(\"üß† Fine-tuning training+validation on Base Classes\")\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Training of one epoch\n",
        "\n",
        "    model.train()\n",
        "    # here we store the sum of all the computed losses through the all batches\n",
        "    total_loss = 0\n",
        "    # here we store the number of correct predictions we will make\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(train_dataloader):\n",
        "\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "          output= model(image)\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "          optimizer.step()\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Training accuracy computation\n",
        "          predicted_class = output.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Training loss: {total_loss/ len(train_dataloader)}; Training accuracy: {correct_predictions / len(train_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    # Validation of one epoch\n",
        "\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          output= model(image)\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = output.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Validation loss: {total_loss/ len(val_dataloader)}; Validation accuracy: {correct_predictions / len(val_dataset)*100:.2f}%\")\n",
        "    print()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "\n",
        "\n",
        "    #evaluation on novel classes\n",
        "\n",
        "    for image, target in tqdm(val_dataloader):\n",
        "\n",
        "          target = torch.Tensor([contig_cat2idx[t.item()] for t in target]).long()\n",
        "\n",
        "          image = image.to(device).float()\n",
        "          target = target.to(device)\n",
        "\n",
        "          # forward image through CLIP image encoder\n",
        "          output= model(image)\n",
        "\n",
        "          loss = criterion(output, target)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          # Validation accuracy computation\n",
        "          predicted_class = output.argmax(dim=-1)\n",
        "          correct_predictions += (predicted_class == target).sum().item()\n",
        "    print(\"-----------------------------------------------------------------------------------------------\")\n",
        "\n",
        "  return(model)\n",
        "\n",
        "model_cocoop = cocoop( train_dataset=train_base, val_dataset=val_base, categories=base_classes, batch_size=1, num_epochs=10, device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "2YuY2yp74JQS",
        "outputId": "f980498b-79e6-407b-d014-40e184050846"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial context: 'a photo of a type of flower, the '\n",
            "Number of context words (tokens): 4\n",
            "Turning off gradients in both the image and the text encoder\n",
            "Total parameters: 149,656,097\n",
            "Total trainable parameters: 149,656,096\n",
            "üß† Fine-tuning training+validation on Base Classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [03:37<00:00,  2.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training loss: 4.63580123536727; Training accuracy: 2.35%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 510/510 [01:09<00:00,  7.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Validation loss: 3.932213209657108; Validation accuracy: 1.96%\n",
            "\n",
            "-----------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|‚ñà‚ñà‚ñà‚ñã      | 187/510 [01:19<02:17,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-d9497a0e25b9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m \u001b[0mmodel_cocoop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcocoop\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-d9497a0e25b9>\u001b[0m in \u001b[0;36mcocoop\u001b[0;34m(train_dataset, val_dataset, categories, batch_size, num_epochs, device, n_ctx, ctx_init, class_token_position, csc)\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m           \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-9d09c6f5b7c7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, label)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimf_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_prompts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_features\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0ml_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit_scale\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimf_i\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-1da478d0431e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, prompts, tokenized_prompts)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# x.shape = [batch_size, n_ctx, transformer.width]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# take features from the eot embedding (eot_token is the highest number in each sequence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_prompts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_projection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1913\u001b[0m     \u001b[0;31m# on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1914\u001b[0m     \u001b[0;31m# https://github.com/pytorch/pytorch/pull/115074\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Module\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"_parameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_parameters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}